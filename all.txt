
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <time.h>
#include <unistd.h>

#include "./libs/structs.h"
#include "./libs/optimizer.h"
#include "./libs/train.h"
#include "./libs/utils.h"
#include "./libs/metrics.h"

#define NUM_LAYERS 4

int main(int argc, char *argv[]) {
    srand(251);

    char *dataset = argv[1];

    int train_rows = atoi(argv[2]),
        test_rows = atoi(argv[3]),
        columns = atoi(argv[4]),
        batch_size = atoi(argv[5]),
        epochs = atoi(argv[6]);

    double precision = atof(argv[7]),
            base_lr = atof(argv[8]);

    Matrix X_train, y_train, X_test, y_test;
    char **paths = get_paths(dataset);

    get_data_from_csv(paths[0], &X_train, &y_train, train_rows, columns);
    get_data_from_csv(paths[1], &X_test, &y_test, test_rows, columns);

    free(paths[0]);
    free(paths[1]);
    free(paths);

    int n_batches;

    if (batch_size > 0) {
        n_batches = (int) (train_rows / batch_size);
        if (n_batches * batch_size < train_rows) n_batches += 1;
    } else {
        batch_size = train_rows;
        n_batches = 1;
    }

    Matrix *X_train_batches = to_batches(X_train, batch_size, n_batches);
    Matrix *y_train_batches = to_batches(y_train, batch_size, n_batches);

    Layer layers[NUM_LAYERS] = {
        init_layer(X_train.dims[1], 16, "relu", 0, 5e-4, 0, 5e-4),
        init_layer(16, 12, "relu", 0, 5e-4, 0, 5e-4),
        init_layer(12, 8, "relu", 0, 5e-4, 0, 5e-4),
        init_layer(8, 1, "linear", 0, 0, 0, 0),
    };

    double lr = base_lr;
    double decay_rate = 9e-4;
    double epsilon = 1e-7;
    double beta_1 = 0.93;
    double beta_2 = 0.997;

    Matrix output, grad, error, dX, dW, dB;

    int step = 0;
    for (int i = 0; i < epochs; i++) {
        double acc = 0, loss = 0;
        printf("==== Epoch %i ====\n", i + 1);
        for (int j = 0; j < n_batches; j++) {
            double batch_acc = 0, batch_loss = 0;
            output = X_train_batches[j];
            for (int k = 0; k < NUM_LAYERS; k++) {
                output = forward(output, &layers[k]);
            }

            error = Error(y_train_batches[j], output);
            Metrics_and_Gradients(error, &batch_acc, &batch_loss, &grad, precision);
            // printf("\t== Batch %i ==\n", j + 1);
            // printf("\tAccuracy: %.5lf\n", batch_acc);
            // printf("\tLoss: %.5lf\n", batch_loss);

            pre_update(base_lr, &lr, decay_rate, i);
            for (int k = NUM_LAYERS - 1; k >= 0; k--) {
                dX = backward(grad, &layers[k], &dW, &dB);
                update_params_Adam(&layers[k], dW, dB, lr, beta_1, beta_2, epsilon, i);
                grad = dX;
            }

            step++;
            acc += batch_acc;
            loss += batch_loss;
        }
        acc /= n_batches;
        loss /= n_batches;
        printf("Accuracy: %.5lf\n", acc);
        printf("Loss: %.5lf\n", loss);
        write_metrics(i + 1, acc, loss, dataset);
    }

    double acc = 0, loss = 0;
    printf("======== TEST ========\n");
    output = X_test;
    for (int i = 0; i < NUM_LAYERS; i++) {
        output = forward(output, &layers[i]);
    }
    error = Error(y_test, output);
    Metrics_and_Gradients(error, &acc, &loss, &grad, precision);
    printf("Accuracy: %.5lf\n", acc);
    printf("Loss: %.5lf\n", loss);

    free(X_train.data);
    free(y_train.data);
    free(X_test.data);
    free(y_test.data);
    for (int i = 0; i < NUM_LAYERS; i++) {
        free(layers[i].weights.data);
        free(layers[i].biases.data);
    }

    return 0;
}

=================================================================

#ifndef ACTIVATION_FUNCTIONS_H
#define ACTIVATION_FUNCTIONS_H

#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#include "structs.h"
#include "utils.h"

Matrix Linear(Matrix z, Layer *layer) {
    layer -> z = z;
    layer -> outputs = z;
    return layer -> outputs;
}

Matrix ReLU(Matrix z, Layer *layer) {
    double *data = double_vector(z.dims[0] * z.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < z.dims[0] * z.dims[1]; i++) {
        data[i] = (z.data[i] > 0) ? z.data[i] : 0;
    }

    layer -> z = z;
    layer -> outputs = init_matrix(data, z.dims[0], z.dims[1]);
    return layer -> outputs;
}

Matrix Leaky_ReLU(Matrix z, Layer *layer) {
    double *data = double_vector(z.dims[0] * z.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < z.dims[0] * z.dims[1]; i++) {
        data[i] = z.data[i] * ((data[i] > 0) ? 1 : 0.01);
    }

    layer -> z = z;
    layer -> outputs = init_matrix(data, z.dims[0], z.dims[1]);
    return layer -> outputs;
}

Matrix Sigmoid(Matrix z, Layer *layer) {
    double *data = double_vector(z.dims[0] * z.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < z.dims[0] * z.dims[1]; i++) {
        data[i] = 1 / (1 + exp(-z.data[i]));
    }

    layer -> z = z;
    layer -> outputs = init_matrix(data, z.dims[0], z.dims[1]);
    return layer -> outputs;
}

Matrix Tanh(Matrix z, Layer *layer) {
    double *data = double_vector(z.dims[0] * z.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < z.dims[0] * z.dims[1]; i++) {
        data[i] = tanh(z.data[i]);
    }

    layer -> z = z;
    layer -> outputs = init_matrix(data, z.dims[0], z.dims[1]);
    return layer -> outputs;
}

Matrix dLinear(Matrix grad, Layer layer) {
    double *data = double_vector(layer.outputs.dims[0] * layer.outputs.dims[1]);

    return init_matrix(data, layer.outputs.dims[0], layer.outputs.dims[1]);
}

Matrix dReLU(Matrix grad, Layer layer) {
    double *data = double_vector(layer.outputs.dims[0] * layer.outputs.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < layer.outputs.dims[0] * layer.outputs.dims[1]; i++) {
        data[i] = layer.z.data[i] > 0 ? grad.data[i] : 0;
    }

    return init_matrix(data, layer.outputs.dims[0], layer.outputs.dims[1]);
}

Matrix dLeaky_ReLU(Matrix grad, Layer layer) {
    double *data = double_vector(layer.outputs.dims[0] * layer.outputs.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < layer.outputs.dims[0] * layer.outputs.dims[1]; i++) {
        data[i] = grad.data[i] * (layer.z.data[i] > 0 ? 1 : 0.01);
    }

    return init_matrix(data, layer.outputs.dims[0], layer.outputs.dims[1]);
}

Matrix dSigmoid(Matrix grad, Layer layer) {
    double *data = double_vector(layer.outputs.dims[0] * layer.outputs.dims[1]);

    #pragma omp parallel for
    for (int i = 0; i < layer.outputs.dims[0] * layer.outputs.dims[1]; i++) {
        data[i] = grad.data[i] * layer.outputs.data[i] * (1 - layer.outputs.data[i]);
    }

    return init_matrix(data, layer.outputs.dims[0], layer.outputs.dims[1]);
}

Matrix dTanh(Matrix grad, Layer layer) {
    double *data = double_vector(layer.outputs.dims[0] * layer.outputs.dims[1]);

    for (int i = 0; i < layer.outputs.dims[0] * layer.outputs.dims[1]; i++) {
        data[i] = grad.data[i] * (1 - layer.outputs.data[i] * layer.outputs.data[i]);
    }

    return init_matrix(data, layer.outputs.dims[0], layer.outputs.dims[1]);
}

#endif
=======================================================================
#include <stdio.h>
#include <stdlib.h>

#include "structs.h"
#include "utils.h"

Matrix Error(Matrix y, Matrix output) {
    int n = output.dims[0];
    double *data = double_vector(n);

    for (int i = 0; i < n; i++) {
        data[i] = y.data[i] - output.data[i];
    }

    return init_matrix(data, n, 1);
}

void Metrics_and_Gradients(Matrix error, double *acc, double *loss, Matrix *mse_grad, double prec) {
    double total_acc = 0;
    double total_loss = 0;

    int n = error.dims[0];
    double *grad = double_vector(n);

    for (int i = 0; i < n; i++) {
        total_acc += abs(error.data[i]) < prec;
        total_loss += error.data[i] * error.data[i];
        grad[i] = -(2.0 / n) * error.data[i];
    }

    *acc = total_acc / n;
    *loss = total_loss / n;
    *mse_grad = init_matrix(grad, n, 1);
}

=====================================================================

#ifndef NN_OPTIMIZER_H
#define NN_OPTIMIZER_H

#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#include "structs.h"

void pre_update(double base_lr, double *lr, double decay_rate, int epoch) {
    *lr = base_lr / (1 + decay_rate * epoch);
}

void update_params_SGD(Layer *layer, Matrix dW, Matrix dB, double lr, double momentum) {
    #pragma omp parallel for
    for (int j = 0; j < layer -> weights.dims[1]; j++) {
        for (int i = 0; i < layer -> weights.dims[0]; i++) {
            int idx = i * layer -> weights.dims[1] + j;
            double weight_update = momentum * layer -> weight_momentums.data[idx] - lr * dW.data[idx];
            layer -> weight_momentums.data[idx] = weight_update;
            layer -> weights.data[idx] += weight_update;
        }
        double bias_update = momentum * layer -> bias_momentums.data[j] - lr * dB.data[j];
        layer -> bias_momentums.data[j] = bias_update;
        layer -> biases.data[j] += bias_update;
    }
}

void update_params_Adam(Layer *layer, Matrix dW, Matrix dB, double lr, double beta_1, double beta_2, double epsilon, int epoch) {
    #pragma omp parallel for
    for (int j = 0; j < layer -> weights.dims[1]; j++) {
        for (int i = 0; i < layer -> weights.dims[0]; i++) {
            int idx = i * layer -> weights.dims[1] + j;

            layer -> weight_momentums.data[idx] = beta_1 * layer -> weight_momentums.data[idx] + (1 - beta_1) * dW.data[idx];
            double weight_momentum_corrected = layer -> weight_momentums.data[idx] / (1 - pow(beta_1, epoch + 1));

            layer -> weight_cache.data[idx] = beta_2 * layer -> weight_cache.data[idx] + (1 - beta_2) * dW.data[idx] * dW.data[idx];
            double weight_cache_corrected = layer -> weight_cache.data[idx] / (1 - pow(beta_2, epoch + 1));

            layer -> weights.data[idx] += - lr * weight_momentum_corrected / (sqrt(weight_cache_corrected) + epsilon);
        }

        layer -> bias_momentums.data[j] = beta_1 * layer -> bias_momentums.data[j] + (1 - beta_1) * dB.data[j];
        double bias_momentum_corrected = layer -> bias_momentums.data[j] / (1 - pow(beta_1, epoch + 1));

        layer -> bias_cache.data[j] = beta_2 * layer -> bias_cache.data[j] + (1 - beta_2) * dB.data[j] * dB.data[j];
        double bias_cache_corrected = layer -> bias_cache.data[j] / (1 - pow(beta_2, epoch + 1));

        layer -> biases.data[j] += - lr * bias_momentum_corrected / (sqrt(bias_cache_corrected) + epsilon);
    }
}

#endif
==================================================================


#ifndef NN_STRUCTS_H
#define NN_STRUCTS_H

#include <stdio.h>
#include <stdlib.h>

#define PI 3.14159265358979323846

typedef struct {
    double *data;
    int dims[2];
} Matrix;

typedef struct {
    Matrix weights;
    Matrix biases;
    const char *activation;
    Matrix inputs;
    Matrix z;
    Matrix outputs;

    Matrix weight_momentums;
    Matrix bias_momentums;

    Matrix weight_cache;
    Matrix bias_cache;

    double weight_regularizer_l1;
    double weight_regularizer_l2;
    double bias_regularizer_l1;
    double bias_regularizer_l2;
} Layer;

Matrix init_matrix(double *data, int rows, int columns) {
    Matrix m;
    m.data = data;
    m.dims[0] = rows;
    m.dims[1] = columns;

    return m;
}

double random_weight() {
    double u1 = (rand() + 1.0) / (RAND_MAX + 2.0);
    double u2 = (rand() + 1.0) / (RAND_MAX + 2.0);
    return sqrt(-2.0 * log(u1)) * cos(2.0 * PI * u2);
}

Layer init_layer(int input_size, int size, const char *activation,
        double weight_regularizer_l1,
        double weight_regularizer_l2,
        double bias_regularizer_l1,
        double bias_regularizer_l2) {
    Layer layer;

    double *weights = (double *) malloc(input_size * size * sizeof(double));
    for (int i = 0; i < input_size * size; i++) {
        weights[i] = random_weight();
    }

    layer.weight_regularizer_l1 = weight_regularizer_l1;
    layer.weight_regularizer_l2 = weight_regularizer_l2;
    layer.bias_regularizer_l1 = bias_regularizer_l1;
    layer.bias_regularizer_l2 = bias_regularizer_l2;

    double *biases = (double *) calloc(size, sizeof(double));

    double *weight_mom = (double *) calloc(input_size * size, sizeof(double));
    double *weight_cac = (double *) calloc(input_size * size, sizeof(double));

    double *bias_mom = (double *) calloc(size, sizeof(double));
    double *bias_cac = (double *) calloc(size, sizeof(double));

    layer.weights = init_matrix(weights, input_size, size);
    layer.weight_momentums = init_matrix(weight_mom, input_size, size);
    layer.weight_cache = init_matrix(weight_cac, input_size, size);

    layer.biases = init_matrix(biases, size, 1);
    layer.bias_momentums = init_matrix(bias_mom, size, 1);
    layer.bias_cache = init_matrix(bias_cac, size, 1);

    layer.activation = activation;

    return layer;
}

#endif
=================================================================================

#ifndef NN_TRAIN_H
#define NN_TRAIN_H

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include "structs.h"
#include "activation_functions.h"

Matrix *to_batches(Matrix X, int batch_size, int n) {
    int start = 0,
        end = 0;
    int n_columns = X.dims[1];
    Matrix *batches = (Matrix *) malloc(n * sizeof(Matrix));
    for (int i = 0; i < n; i++) {
        start = i * batch_size;
        end = (i < n - 1) ? (i + 1) * batch_size : X.dims[0];
        double *data = (double *) malloc((end - start) * n_columns * sizeof(double));
        for (int j = start; j < end; j++) {
            for (int k = 0; k < n_columns; k++) {
                data[(j - start) * n_columns + k] = X.data[j * n_columns + k];
            }
        }
        batches[i] = init_matrix(data, end - start, n_columns);
    }

    return batches;
}

Matrix choose_activation_function(Matrix z, Layer *layer) {
    if (strcmp(layer -> activation, "relu") == 0) {
        return ReLU(z, layer);
    } else if (strcmp(layer -> activation, "sigmoid") == 0) {
        return Sigmoid(z, layer);
    } else if (strcmp(layer -> activation, "tanh") == 0) {
        return Tanh(z, layer);
    } else if (strcmp(layer -> activation, "leaky_relu") == 0) {
        return Leaky_ReLU(z, layer);
    } else return Linear(z, layer);
}

Matrix forward(Matrix inputs, Layer *layer) {
    layer -> inputs = inputs;
    double *data = (double *) malloc(inputs.dims[0] * layer -> weights.dims[1] * sizeof(double));

    int layer_dims[2];
    layer_dims[0] = layer -> weights.dims[0];
    layer_dims[1] = layer -> weights.dims[1];

    int inputs_dims[2];
    inputs_dims[0] = inputs.dims[0];
    inputs_dims[1] = inputs.dims[1];

    #pragma omp parallel for
    for (int i = 0; i < inputs_dims[0]; i++) {
        for (int j = 0; j < layer_dims[1]; j++) {
            data[i * layer_dims[1] + j] = layer -> biases.data[j];
            for (int k = 0; k < inputs.dims[1]; k++) {
                data[i * layer_dims[1] + j] += inputs.data[i * inputs_dims[1] + k] * layer -> weights.data[k * layer_dims[1] + j];
            }
        }
    }

    Matrix z = init_matrix(data, inputs.dims[0], layer -> weights.dims[1]);
    return choose_activation_function(z, layer);
}

Matrix calculate_dX(Matrix grad, Layer layer) {
    double *dx = double_vector(grad.dims[0] * layer.weights.dims[0]);

    #pragma omp parallel for
    for (int i = 0; i < grad.dims[0]; i++) {
        for (int j = 0; j < layer.weights.dims[0]; j++) {
            dx[i * layer.weights.dims[0] + j] = 0;
            for (int k = 0; k < grad.dims[1]; k++) {
                dx[i * layer.weights.dims[0] + j] += grad.data[i * grad.dims[1] + k] * layer.weights.data[j * layer.weights.dims[1] + k];
            }
        }
    }

    return init_matrix(dx, grad.dims[0], layer.weights.dims[0]);
}

Matrix calculate_dW(Layer layer, Matrix grad) {
    double *dw = double_vector(layer.inputs.dims[1] * grad.dims[1]);
    double dL1;

    #pragma omp parallel for
    for (int i = 0; i < layer.inputs.dims[1]; i++) {
        for (int j = 0; j < grad.dims[1]; j++) {
            int idx = i * layer.weights.dims[1] + j;
            dw[idx] = 0;
            for (int k = 0; k < grad.dims[0]; k++) {
                dw[idx] += layer.inputs.data[k * layer.inputs.dims[1] + i] * grad.data[k * grad.dims[1] + j];
            }

            if (layer.weight_regularizer_l1 > 0) {
                dL1 = layer.weights.data[idx] < 0 ? -1 : 1;
                dw[idx] += layer.weight_regularizer_l1 * dL1;
            }

            if (layer.weight_regularizer_l2 > 0) {
                dw[idx] += 2 * layer.weight_regularizer_l2 * layer.weights.data[idx];
            }
        }
    }

    return init_matrix(dw, layer.inputs.dims[1], grad.dims[1]);
}

Matrix calculate_dB(Layer layer, Matrix grad) {
    double *db = double_vector(grad.dims[1]);
    double dL1;

    #pragma omp parallel for
    for (int i = 0; i < grad.dims[1]; i++) {
        db[i] = 0;
        for (int j = 0; j < grad.dims[0]; j++) {
            db[i] += grad.data[j * grad.dims[1] + i];
        }

        if (layer.bias_regularizer_l1 > 0) {
            dL1 = layer.biases.data[i] < 0 ? -1 : 1;
            db[i] += layer.bias_regularizer_l1 * dL1;
        }

        if (layer.bias_regularizer_l2 > 0) {
            db[i] += 2 * layer.bias_regularizer_l2 * layer.biases.data[i];
        }
    }

    return init_matrix(db, grad.dims[1], 1);
}

Matrix backward(Matrix grad, Layer *layer, Matrix *dW, Matrix *dB) {
    if (strcmp(layer -> activation, "relu") == 0) {
        grad = dReLU(grad, *layer);
    } else if (strcmp(layer -> activation, "sigmoid") == 0) {
        grad = dSigmoid(grad, *layer);
    } else if (strcmp(layer -> activation, "tanh") == 0) {
        grad = dTanh(grad, *layer);
    } else grad = dLeaky_ReLU(grad, *layer);

    *dW = calculate_dW(*layer, grad);
    *dB = calculate_dB(*layer, grad);

    return calculate_dX(grad, *layer);
}

#endif

================================================================

#ifndef NN_UTILS_H
#define NN_UTILS_H

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include "structs.h"

double *double_vector(int size) {
    return (double *) malloc(size * sizeof(double));
}

void show_matrix(Matrix m, int limit) {
    if (limit == 0) return;

    int end = (limit > 0 && limit <= m.dims[0]) ? limit : m.dims[0];

    for (int i = 0; i < end; i++) {
        printf("[");
        for (int j = 0; j < m.dims[1]; j++) {
            printf("%.10f, ", m.data[i * m.dims[1] + j]);
        }
        printf("],\n");
    }
    printf("\nSize: %i x %i\n============\n", m.dims[0], m.dims[1]);
}

char **get_paths(char *dataset) {
    static const char *base_dir = "./datasets/";

    char **paths = (char **) malloc(2 * sizeof(char *));
    paths[0] = (char *) malloc(512 * sizeof(char));
    paths[1] = (char *) malloc(512 * sizeof(char));

    sprintf(paths[0], "%s%s/train.csv", base_dir, dataset);
    sprintf(paths[1], "%s%s/test.csv", base_dir, dataset);

    return paths;
}

void write_metrics(int epoch, double acc, double loss, char *dataset) {
    static const char *base_dir = "./datasets/";
    char *file_path = (char *) malloc(512 * sizeof(char));
    sprintf(file_path, "%s%s/metrics.csv", base_dir, dataset);

    FILE *arquivo = fopen(file_path, "a");

    if (arquivo == NULL) {
        printf("Erro ao abrir o arquivo!\n");
        return;
    }

    fprintf(arquivo, "%i,%f,%f\n", epoch, acc, loss);
    fclose(arquivo);
}

void get_data_from_csv(char *csv, Matrix *X, Matrix *y, int rows, int columns) {
    FILE *file;
    char row[1024];

    file = fopen(csv, "r");
    if (file == NULL) {
        printf("Error when opening the file.\n");
        return;
    }

    char *token = strtok(row, ",\n");

    double *X_, *y_;
    X_ = (double *) malloc((columns - 1) * rows * sizeof(double));
    y_ = (double *) malloc(rows * sizeof(double));

    int i = 0;
    while (fgets(row, sizeof(row), file) && i < rows) {
        token = strtok(row, ",\n");
        int j = 0;
        while (token != NULL && j < columns) {
            double value = atof(token);
            if (j < columns - 1) X_[i * (columns - 1) + j] = value;
            else y_[i] = value;
            token = strtok(NULL, ",\n");
            j++;
        }

        i++;
    }

    *X = init_matrix(X_, rows, columns - 1);
    *y = init_matrix(y_, rows, 1);

    fclose(file);
}

#endif

